# -*- coding: utf-8 -*-
"""falcon_positive_reframing_v7_server

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JhIMU7glS_Azi83O-axcAgsG86XoSHYi

## Finetune Falcon-7b on a Google colab

Welcome to this Google Colab notebook that shows how to fine-tune the recent Falcon-7b model on a single Google colab and turn it into a chatbot

We will leverage PEFT library from Hugging Face ecosystem, as well as QLoRA for more memory efficient finetuning

## Setup

Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes). We will also install `einops` as it is a requirement to load Falcon models.

# Import Libaries


* PEFT code is refered to
https://medium.com/@amodwrites/a-definitive-guide-to-qlora-fine-tuning-falcon-7b-with-peft-78f500a1f337
"""

import argparse
import pandas as pd
import numpy as np
import torch
import random
# from sentence_transformers import SentenceTransformer, util
from transformers import DataCollatorForLanguageModeling
import os
from transformers import Trainer, pipeline, set_seed, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainingArguments
import nltk
nltk.download('punkt')
import csv
import transformers
from datasets import load_dataset, load_metric
from peft import LoraConfig,PeftConfig,get_peft_model,prepare_model_for_kbit_training
from transformers import AutoConfig,AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig
from peft import AutoPeftModelForCausalLM

from datasets import load_dataset
root = "./"
train_path = root + "data/wholetrain_gpt.txt"
# train_path = root + "data/wholetrain.csv"
# train_dataset = load_dataset('csv', data_files=train_path)
dev_path =  root+"data/wholedev.csv"
dev_dataset = load_dataset('csv', data_files=dev_path)
test_path =  root + "data/wholetest.csv"
test_dataset = load_dataset('csv', data_files=test_path)


def run_falcon_unconstrained(name="s"):
    if name == "s":
        model_name = "Rocketknight1/falcon-rw-1b"
        output_dir = root+"falcon-rw-1b"
        f_name = "falcon_rw_1b_predict.txt"
    else:
        model_name = "tiiuae/falcon-7b"
        output_dir = root+"falcon-7b"
        f_name = "falcon_7b_predict.txt"

    bnb_config = BitsAndBytesConfig(
      load_in_4bit=True,
      load_4bit_use_double_quant=True,
      bnb_4bit_quant_type="nf4",
      bnb_4bit_compute_dtype=torch.float16,
    )
    from transformers import AutoModelForCausalLM
    model =AutoModelForCausalLM.from_pretrained(
      model_name,
    #   device_map="auto",
    #   device_map="cpu",
      trust_remote_code=True,
      quantization_config=bnb_config,
    )
    model = prepare_model_for_kbit_training(model)
    config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[
        "query_key_value",
        "dense",
        "dense_h_to_4h",
        "dense_4h_to_h",],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
    )

    model = get_peft_model(model, config)

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.bos_token = "<startoftext>"
    tokenizer.eos_token = "<endoftext>"
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

    metric = load_metric("rouge")
    metric2 = load_metric("sacrebleu")

    from transformers import TextDataset

    def load_dataset(train_path,test_path,tokenizer):
        train_dataset = TextDataset(
            tokenizer=tokenizer,
            file_path=train_path,
            block_size=30)
        test_dataset = TextDataset(
            tokenizer=tokenizer,
            file_path=test_path,
            block_size=30)
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer, mlm=False,
        )
        return train_dataset,test_dataset,data_collator

    print("---------------------------------------")
    print(train_path)
    print(test_path)
    print("---------------------------------------")
    train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)

    batch_size = 4
    optim = "paged_adamw_32bit"
    per_device_train_batch_size = batch_size
    gradient_accumulation_steps = batch_size
    # save_steps = 10
    logging_steps = 10
    learning_rate = 2e-4
    max_grad_norm = 0.3
    max_steps = 1   # epoch
    warmup_ratio = 0.03
    lr_scheduler_type = "constant"
    args = transformers.TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        eval_accumulation_steps = gradient_accumulation_steps,
        optim=optim,
        learning_rate=learning_rate,
        # evaluation_strategy="epoch",    # no for no testing
        evaluation_strategy = "no",
        do_train=True,
        do_eval=True,
        logging_steps=1024,
        save_steps=2048,
        # warmup_steps=1024,
        max_grad_norm=max_grad_norm,
        warmup_ratio=warmup_ratio,
        # group_by_length=True,
        # predict_with_generate=True,   # ?? why... not working?
        lr_scheduler_type=lr_scheduler_type,
        #max_steps=1500, # delete for full training
        num_train_epochs = max_steps, #TRAIN_EPOCHS
        overwrite_output_dir=True,
        save_total_limit=5,
        fp16=True,
        # bf16=True,
    )


    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        # Replace -100 in the labels as we can't decode them.
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

        # # Rouge expects a newline after each sentence
        # decoded_preds_joined = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
        # decoded_labels_joined = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]
        # # rouge score
        # result = metric.compute(predictions=decoded_preds_joined, references=decoded_labels_joined, use_stemmer=True)
        # Extract a few results
        # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}

        # Add mean generated length
        result = {}
        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
        result["gen_len"] = np.mean(prediction_lens)
        
        # bleu scores
        decoded_labels_expanded = [[x] for x in decoded_labels]
        result2 = metric2.compute(predictions=decoded_preds, references=decoded_labels_expanded)

        # print(result2)
        result['sacrebleu'] = round(result2["score"], 1)

        return {k: round(v, 4) for k, v in result.items()}

    trainer = transformers.Trainer(
        model=model,
        args=args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        # compute_metrics=compute_metrics
    )
    model.config.use_cache = False
    trainer.train()

    trainer.evaluate()
    # save model
    trainer.save_model(output_dir+"/output/reframer")

    # model = AutoPeftModelForCausalLM.from_pretrained(output_dir+"/output/reframer", load_in_4bit=True)
    model = AutoModelForCausalLM.from_pretrained(output_dir+"/output/reframer", load_in_4bit=True)
    config = PeftConfig.from_pretrained(output_dir+"/output/reframer")
    model = prepare_model_for_kbit_training(model)
    tokenizer = AutoTokenizer.from_pretrained(
        config.base_model_name_or_path)
    tokenizer.bos_token = "<startoftext>"
    tokenizer.eos_token = "<endoftext>"
    tokenizer.pad_token = tokenizer.eos_token

    # num_return_sequences 추가해서 돌려보기
    reframer = pipeline('text-generation', model=model, tokenizer=tokenizer, eos_token_id=tokenizer.eos_token_id,  num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)

    import csv
    with open (test_path, newline="") as data:
      annotations = csv.DictReader(data, delimiter=',', quotechar='"')
      annotations_list = list(annotations)
      reframed_phrases = []
      answer_phrases = []
      for i in range(len(annotations_list)):
          prefix = "<startoftext> " + annotations_list[i]['original_text'] + "\nreframed:"
          print(prefix)
          gen_text = reframer(prefix, max_length=100)[0]['generated_text']
          print("-----------------------------------------------------------------------")
          print(gen_text)
          print("-----------------------------------------------------------------------")
          reframed_phrases.append(gen_text+"\n---------------")
          answer_phrases.append(annotations_list[i]['original_text'] + "\nreframed:"+ annotations_list[i]['reframed_text']+"\n-------------")

    with open(os.path.join(root, f_name), 'w') as f:
        for item in reframed_phrases:
            f.write("%s\n" % item)
    print("write complete!")
    with open(os.path.join(root, "total_reframe.txt"),'w') as f:
      for item in answer_phrases:
        f.write("%s\n"%item)
    print("answer written")

run_falcon_unconstrained()

# # # is it okay to use this?
# # tokenizer = AutoTokenizer.from_pretrained('Rocketknight1/falcon-rw-1b')
# model = AutoPeftModelForCausalLM.from_pretrained("./falcon-rw-1b/output/reframer", load_in_4bit=True)
# config = PeftConfig.from_pretrained("./falcon-rw-1b/output/reframer")
# model = prepare_model_for_kbit_training(model)
# tokenizer = AutoTokenizer.from_pretrained(
#     config.base_model_name_or_path)
# tokenizer.pad_token = tokenizer.eos_token
# tokenizer.bos_token = "<|startoftext|>"

# reframer = pipeline('text-generation', model=model, tokenizer=tokenizer, eos_token_id=tokenizer.eos_token_id,  pad_token_id=tokenizer.eos_token_id)
# import csv
# with open (test_path, newline="") as data:
#   annotations = csv.DictReader(data, delimiter=',', quotechar='"')
#   print(annotations)
#   # for item in annotations:
#   #   print(item)
#   annotations_list = list(annotations)
#   reframed_phrases = []
#   answer_phrases = []
#   for i in range(len(annotations_list)):
#       prefix = "<|startoftext|> " + annotations_list[i]['original_text'] + "\nreframed:"
#       gen_text = reframer(prefix, max_length=100)[0]['generated_text']
#       reframed_phrases.append(gen_text)
#       answer_phrases.append(annotations_list[i]['reframed_text'])

# # test = pd.read_csv(test_path)
# # texts = test['original_text'].to_list()
# # reframed_phrases = [reframer(phrase)[0]['generated_text'] for phrase in texts]

# with open(os.path.join(root, "output_temp.txt"), 'w') as f:
#     for item in reframed_phrases:
#         f.write("%s\n" % item)

# with open(os.path.join(root, "total_reframe.txt"),'w') as f:
#   for item in answer_phrases:
#     f.write("%s\n"%item)